# Agent Instructions

This project automates job searching: scraping listings, filtering, scoring against your profile, and generating tailored application documents. The design separates AI reasoning from deterministic execution — the AI agent (you) orchestrates, while Python scripts in `tools/` do the actual work.

## How to Operate

**1. Look for existing tools first**
Before building anything new, check `tools/`. Only create new scripts when nothing exists for that task.

**2. Learn and adapt when things fail**
When you hit an error:
- Read the full error message and trace
- Fix the script and retest (if it uses paid API calls or credits, check with me before running again)
- Document what you learned in this file or in code comments

**3. Keep documentation current**
When you find better methods, discover constraints, or encounter recurring issues, update the relevant docs. Don't overwrite this file without asking.

## The Pipeline

```
scrape_jobs.py → parse_jobs.py → [analyze_jobs.py] → score_jobs.py → generate_documents.py → generate_report.py
```

Orchestrated by `tools/run_pipeline.py`. Run with: `uv run python tools/run_pipeline.py`

## File Structure

```
config/         # User profile and search filters (YAML, gitignored)
tools/          # Python scripts — the pipeline steps
output/         # Final deliverables (report, tailored applications)
.tmp/           # Intermediate data (scraped/parsed/scored jobs). Regenerated as needed.
.env            # LLM API key — Gemini or Groq (gitignored)
```

**Core principle:** Everything in `.tmp/` is disposable and regenerated by the pipeline. Final outputs live in `output/`. User config lives in `config/`.

## Bottom Line

You sit between what I want and what actually gets done (tools). Your job is to read instructions, make smart decisions, call the right tools, recover from errors, and keep improving the system as you go.

Stay pragmatic. Stay reliable. Keep learning.
