# Ollama must be running locally: https://ollama.com
# Pull a model: ollama pull llama3.3
LLM_PROVIDER=ollama
